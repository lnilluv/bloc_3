{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"data:image/png;base64,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\" alt=\"DSW LOGO\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0eiKSLYG8XvO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Challenge : predict conversions üèÜüèÜ\n",
    "\n",
    "This is the template that shows the different steps of the challenge. In this notebook, all the training/predictions steps are implemented for a very basic model (logistic regression with only one variable). Please use this template and feel free to change the preprocessing/training steps to get the model with the best f1-score ! May the force be with you üß®üß®  \n",
    "\n",
    "**For a detailed description of this project, please refer to *02-Conversion_rate_challenge.ipynb*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AGhdl7Bt2xZd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# import ensemble methods\n",
    "from sklearn.ensemble import AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# import base estimators\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHgro65rxKF7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Read file with labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPh1qPTf3wZU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Choose variables to use in the model, and create train and test sets\n",
    "**From the EDA, we know that the most useful feature is total_pages_visited. Let's create a baseline model by using at first only this feature : in the next cells, we'll make preprocessings and train a simple (univariate) logistic regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set with labels (our train+test) : (284580, 6)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('conversion_data_train.csv')\n",
    "print('Set with labels (our train+test) :', data.shape)\n",
    "mask = data[\"age\"] < 80\n",
    "data = data[mask]\n",
    "\n",
    "features_list = ['country', 'age', 'new_user', 'source', 'total_pages_visited']\n",
    "target_variable = 'converted'\n",
    "numeric_indices = [1,3]\n",
    "categorical_indices = [0,2]\n",
    "\n",
    "X = data.loc[:, features_list]\n",
    "Y = data.loc[:, target_variable]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0, stratify=Y)\n",
    "\n",
    "numeric_features = ['age', 'total_pages_visited']  # Names of numeric columns in X_train/X_test\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())  # standardization\n",
    "])\n",
    "\n",
    "categorical_features = ['country', 'new_user', 'source']  # Names of categorical columns in X_train/X_test\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('encoder', OneHotEncoder(drop='first'))\n",
    "        # first column will be dropped to avoid creating correlations between features\n",
    "    ])\n",
    "\n",
    "# Use ColumnTransformer to make a preprocessor object that describes all the treatments to be done\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search...\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630855840761344\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7631934767594089\n",
      "f1-score on train set :  0.7630855840761344\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7640462985290571\n",
      "...Done.\n",
      "Accuracy on training set :  0.7637482374054608\n",
      "Accuracy on test set :  0.7627215551743854\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'invscaling', 'eta0': 100, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7628177384810781\n",
      "f1-score on train set :  0.7637482374054608\n",
      "f1-score on test set :  0.7627215551743854\n",
      "f1-score on set :  0.7636559655007188\n",
      "...Done.\n",
      "Accuracy on training set :  0.7644013804088133\n",
      "Accuracy on test set :  0.7697798929208804\n",
      "Best hyperparameters :  {'penalty': 'elasticnet', 'loss': 'squared_hinge', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-11}\n",
      "Best validation accuracy :  0.7624986325353742\n",
      "f1-score on train set :  0.7644013804088133\n",
      "f1-score on test set :  0.7697798929208804\n",
      "f1-score on set :  0.7628506079210304\n",
      "...Done.\n",
      "Accuracy on training set :  0.7629073353895669\n",
      "Accuracy on test set :  0.7716346153846153\n",
      "Best hyperparameters :  {'penalty': 'elasticnet', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7631091640201065\n",
      "f1-score on train set :  0.7629073353895669\n",
      "f1-score on test set :  0.7716346153846153\n",
      "f1-score on set :  0.7639365997709878\n",
      "...Done.\n",
      "Accuracy on training set :  0.6981274539414074\n",
      "Accuracy on test set :  0.7023004059539919\n",
      "Best hyperparameters :  {'penalty': 'elasticnet', 'loss': 'modified_huber', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7638748485220642\n",
      "f1-score on train set :  0.6981274539414074\n",
      "f1-score on test set :  0.7023004059539919\n",
      "f1-score on set :  0.7487160215457848\n",
      "...Done.\n",
      "Accuracy on training set :  0.7614383469743727\n",
      "Accuracy on test set :  0.7659831121833534\n",
      "Best hyperparameters :  {'penalty': 'elasticnet', 'loss': 'squared_hinge', 'learning_rate': 'invscaling', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7649886981005746\n",
      "f1-score on train set :  0.7614383469743727\n",
      "f1-score on test set :  0.7659831121833534\n",
      "f1-score on set :  0.7525516554642768\n",
      "...Done.\n",
      "Accuracy on training set :  0.7623132078000402\n",
      "Accuracy on test set :  0.7682119205298014\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'squared_hinge', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7604575075637945\n",
      "f1-score on train set :  0.7623132078000402\n",
      "f1-score on test set :  0.7682119205298014\n",
      "f1-score on set :  0.7661975176673199\n",
      "...Done.\n",
      "Accuracy on training set :  0.7628368413996514\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-11}\n",
      "Best validation accuracy :  0.7629460545297012\n",
      "f1-score on train set :  0.7628368413996514\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7639365997709878\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630662020905923\n",
      "Accuracy on test set :  0.7726454709058189\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7627808749652674\n",
      "f1-score on train set :  0.7630662020905923\n",
      "f1-score on test set :  0.7726454709058189\n",
      "f1-score on set :  0.7639365997709878\n",
      "...Done.\n",
      "Accuracy on training set :  0.7643727932849244\n",
      "Accuracy on test set :  0.7720763723150358\n",
      "Best hyperparameters :  {'penalty': 'elasticnet', 'loss': 'squared_hinge', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7620441147366045\n",
      "f1-score on train set :  0.7643727932849244\n",
      "f1-score on test set :  0.7720763723150358\n",
      "f1-score on set :  0.7643650981319461\n",
      "...Done.\n",
      "Accuracy on training set :  0.7650179021349953\n",
      "Accuracy on test set :  0.7707838479809976\n",
      "Best hyperparameters :  {'penalty': 'elasticnet', 'loss': 'log_loss', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7614908397241791\n",
      "f1-score on train set :  0.7650179021349953\n",
      "f1-score on test set :  0.7707838479809976\n",
      "f1-score on set :  0.7594642529508897\n",
      "...Done.\n",
      "Accuracy on training set :  0.7631737529293606\n",
      "Accuracy on test set :  0.7719087635054022\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7629460545297012\n",
      "f1-score on train set :  0.7631737529293606\n",
      "f1-score on test set :  0.7719087635054022\n",
      "f1-score on set :  0.764011088345185\n",
      "...Done.\n",
      "Accuracy on training set :  0.7583582495243273\n",
      "Accuracy on test set :  0.762660158633313\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7584234357365964\n",
      "f1-score on train set :  0.7583582495243273\n",
      "f1-score on test set :  0.762660158633313\n",
      "f1-score on set :  0.758624908245657\n",
      "...Done.\n",
      "Accuracy on training set :  0.7589430894308943\n",
      "Accuracy on test set :  0.7667071688942891\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7588094669720006\n",
      "f1-score on train set :  0.7589430894308943\n",
      "f1-score on test set :  0.7667071688942891\n",
      "f1-score on set :  0.7592897675269998\n",
      "...Done.\n",
      "Accuracy on training set :  0.7655072080412644\n",
      "Accuracy on test set :  0.7695961995249406\n",
      "Best hyperparameters :  {'penalty': 'elasticnet', 'loss': 'squared_hinge', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7611542007842782\n",
      "f1-score on train set :  0.7655072080412644\n",
      "f1-score on test set :  0.7695961995249406\n",
      "f1-score on set :  0.7611232676878191\n",
      "...Done.\n",
      "Accuracy on training set :  0.7588263197126788\n",
      "Accuracy on test set :  0.7667071688942891\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7585421179924058\n",
      "f1-score on train set :  0.7588263197126788\n",
      "f1-score on test set :  0.7667071688942891\n",
      "f1-score on set :  0.7594411567323531\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630855840761344\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7634282773528762\n",
      "f1-score on train set :  0.7630855840761344\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7640462985290571\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630855840761344\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'elasticnet', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-11}\n",
      "Best validation accuracy :  0.7628324005070695\n",
      "f1-score on train set :  0.7630855840761344\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7639148525598504\n",
      "...Done.\n",
      "Accuracy on training set :  0.7627857094979557\n",
      "Accuracy on test set :  0.773109243697479\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7630484469648698\n",
      "f1-score on train set :  0.7627857094979557\n",
      "f1-score on test set :  0.773109243697479\n",
      "f1-score on set :  0.7639365997709878\n",
      "...Done.\n",
      "Accuracy on training set :  0.7588590012873502\n",
      "Accuracy on test set :  0.7667071688942891\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7584446597494627\n",
      "f1-score on train set :  0.7588590012873502\n",
      "f1-score on test set :  0.7667071688942891\n",
      "f1-score on set :  0.7586417491144497\n",
      "...Done.\n",
      "Accuracy on training set :  0.7588970388481391\n",
      "Accuracy on test set :  0.7631257631257632\n",
      "Best hyperparameters :  {'penalty': 'elasticnet', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7583224296270208\n",
      "f1-score on train set :  0.7588970388481391\n",
      "f1-score on test set :  0.7631257631257632\n",
      "f1-score on set :  0.7586375588577021\n",
      "...Done.\n",
      "Accuracy on training set :  0.7486518663771973\n",
      "Accuracy on test set :  0.7518796992481204\n",
      "Best hyperparameters :  {'penalty': 'elasticnet', 'loss': 'hinge', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-11}\n",
      "Best validation accuracy :  0.7633763680746294\n",
      "f1-score on train set :  0.7486518663771973\n",
      "f1-score on test set :  0.7518796992481204\n",
      "f1-score on set :  0.7623270025762385\n",
      "...Done.\n",
      "Accuracy on training set :  0.7592467145373256\n",
      "Accuracy on test set :  0.7662416514875532\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7585475525496621\n",
      "f1-score on train set :  0.7592467145373256\n",
      "f1-score on test set :  0.7662416514875532\n",
      "f1-score on set :  0.7600024359052432\n",
      "...Done.\n",
      "Accuracy on training set :  0.758391085745346\n",
      "Accuracy on test set :  0.7631257631257632\n",
      "Best hyperparameters :  {'penalty': 'elasticnet', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7583895810526469\n",
      "f1-score on train set :  0.758391085745346\n",
      "f1-score on test set :  0.7631257631257632\n",
      "f1-score on set :  0.7586375588577021\n",
      "...Done.\n",
      "Accuracy on training set :  0.7614641995172968\n",
      "Accuracy on test set :  0.76580373269115\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'invscaling', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7648588009131635\n",
      "f1-score on train set :  0.7614641995172968\n",
      "f1-score on test set :  0.76580373269115\n",
      "f1-score on set :  0.7681962025316457\n",
      "...Done.\n",
      "Accuracy on training set :  0.7632565613283342\n",
      "Accuracy on test set :  0.7719087635054022\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7630574005111103\n",
      "f1-score on train set :  0.7632565613283342\n",
      "f1-score on test set :  0.7719087635054022\n",
      "f1-score on set :  0.7640286902537521\n",
      "...Done.\n",
      "Accuracy on training set :  0.7566350549225627\n",
      "Accuracy on test set :  0.7541385652973636\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'squared_hinge', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.760503733696416\n",
      "f1-score on train set :  0.7566350549225627\n",
      "f1-score on test set :  0.7541385652973636\n",
      "f1-score on set :  0.7684463458946258\n",
      "...Done.\n",
      "Accuracy on training set :  0.6841157942102895\n",
      "Accuracy on test set :  0.686406460296097\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'constant', 'eta0': 1, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.6950357591200647\n",
      "f1-score on train set :  0.6841157942102895\n",
      "f1-score on test set :  0.686406460296097\n",
      "f1-score on set :  0.5737245091350163\n",
      "...Done.\n",
      "Accuracy on training set :  0.7678121349972336\n",
      "Accuracy on test set :  0.7637969094922737\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'squared_hinge', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7619710166138021\n",
      "f1-score on train set :  0.7678121349972336\n",
      "f1-score on test set :  0.7637969094922737\n",
      "f1-score on set :  0.7670310192023634\n",
      "...Done.\n",
      "Accuracy on training set :  0.7029094181163767\n",
      "Accuracy on test set :  0.7116154873164219\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'invscaling', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7515183736145103\n",
      "f1-score on train set :  0.7029094181163767\n",
      "f1-score on test set :  0.7116154873164219\n",
      "f1-score on set :  0.679250501279126\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630855840761344\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7630607747149147\n",
      "f1-score on train set :  0.7630855840761344\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7637657559857669\n",
      "...Done.\n",
      "Accuracy on training set :  0.7705231004200077\n",
      "Accuracy on test set :  0.7680911680911682\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'invscaling', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7622484340281832\n",
      "f1-score on train set :  0.7705231004200077\n",
      "f1-score on test set :  0.7680911680911682\n",
      "f1-score on set :  0.7633697872851821\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630026809651475\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7631111338555578\n",
      "f1-score on train set :  0.7630026809651475\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7641208029417085\n",
      "...Done.\n",
      "Accuracy on training set :  0.7347286271727564\n",
      "Accuracy on test set :  0.7389100126742714\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 0.01}\n",
      "Best validation accuracy :  0.7342986777736682\n",
      "f1-score on train set :  0.7347286271727564\n",
      "f1-score on test set :  0.7389100126742714\n",
      "f1-score on set :  0.7357949110388368\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630026809651475\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7630797911963507\n",
      "f1-score on train set :  0.7630026809651475\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7640462985290571\n",
      "...Done.\n",
      "Accuracy on training set :  0.7592404892222449\n",
      "Accuracy on test set :  0.766060606060606\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7469471604811734\n",
      "f1-score on train set :  0.7592404892222449\n",
      "f1-score on test set :  0.766060606060606\n",
      "f1-score on set :  0.7699965671129421\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630026809651475\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7630278730431674\n",
      "f1-score on train set :  0.7630026809651475\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7638754826254827\n",
      "...Done.\n",
      "Accuracy on training set :  0.7587799741865363\n",
      "Accuracy on test set :  0.762660158633313\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7583547662215732\n",
      "f1-score on train set :  0.7587799741865363\n",
      "f1-score on test set :  0.762660158633313\n",
      "f1-score on set :  0.7586375588577021\n",
      "...Done.\n",
      "Accuracy on training set :  0.6922669891904151\n",
      "Accuracy on test set :  0.7002688172043011\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 0.01}\n",
      "Best validation accuracy :  0.6911498343575018\n",
      "f1-score on train set :  0.6922669891904151\n",
      "f1-score on test set :  0.7002688172043011\n",
      "f1-score on set :  0.6921559289747601\n",
      "...Done.\n",
      "Accuracy on training set :  0.7698467313106037\n",
      "Accuracy on test set :  0.7659574468085107\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7613799105195337\n",
      "f1-score on train set :  0.7698467313106037\n",
      "f1-score on test set :  0.7659574468085107\n",
      "f1-score on set :  0.7640855679421512\n",
      "...Done.\n",
      "Accuracy on training set :  0.7590099279205766\n",
      "Accuracy on test set :  0.758113900796081\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'invscaling', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7579074431839778\n",
      "f1-score on train set :  0.7590099279205766\n",
      "f1-score on test set :  0.758113900796081\n",
      "f1-score on set :  0.7669885656733219\n",
      "...Done.\n",
      "Accuracy on training set :  0.7614287618286019\n",
      "Accuracy on test set :  0.7674835624626418\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7612621582061617\n",
      "f1-score on train set :  0.7614287618286019\n",
      "f1-score on test set :  0.7674835624626418\n",
      "f1-score on set :  0.7659700432182819\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630662020905923\n",
      "Accuracy on test set :  0.7719087635054022\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7629969350403546\n",
      "f1-score on train set :  0.7630662020905923\n",
      "f1-score on test set :  0.7719087635054022\n",
      "f1-score on set :  0.7640462985290571\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630026809651475\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.76291490566785\n",
      "f1-score on train set :  0.7630026809651475\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7639365997709878\n",
      "...Done.\n",
      "Accuracy on training set :  0.757265656979124\n",
      "Accuracy on test set :  0.7593386405388854\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'squared_hinge', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7570688488518804\n",
      "f1-score on train set :  0.757265656979124\n",
      "f1-score on test set :  0.7593386405388854\n",
      "f1-score on set :  0.7577449236243176\n",
      "...Done.\n",
      "Accuracy on training set :  0.7582365328442361\n",
      "Accuracy on test set :  0.7631257631257632\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.758323515776903\n",
      "f1-score on train set :  0.7582365328442361\n",
      "f1-score on test set :  0.7631257631257632\n",
      "f1-score on set :  0.7586375588577021\n",
      "...Done.\n",
      "Accuracy on training set :  0.7588075880758807\n",
      "Accuracy on test set :  0.7667071688942891\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7585119060233764\n",
      "f1-score on train set :  0.7588075880758807\n",
      "f1-score on test set :  0.7667071688942891\n",
      "f1-score on set :  0.7598951027627004\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630026809651475\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7629462827006596\n",
      "f1-score on train set :  0.7630026809651475\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.763943322279168\n",
      "...Done.\n",
      "Accuracy on training set :  0.7631931422448432\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7628636572620298\n",
      "f1-score on train set :  0.7631931422448432\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7640395275970113\n",
      "...Done.\n",
      "Accuracy on training set :  0.7340674955595027\n",
      "Accuracy on test set :  0.7389100126742714\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'squared_hinge', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 0.01}\n",
      "Best validation accuracy :  0.7338305123593087\n",
      "f1-score on train set :  0.7340674955595027\n",
      "f1-score on test set :  0.7389100126742714\n",
      "f1-score on set :  0.7345008308832929\n",
      "...Done.\n",
      "Accuracy on training set :  0.7143173268422217\n",
      "Accuracy on test set :  0.7259646827992151\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'invscaling', 'eta0': 1, 'alpha': 0.01}\n",
      "Best validation accuracy :  0.7376999927275678\n",
      "f1-score on train set :  0.7143173268422217\n",
      "f1-score on test set :  0.7259646827992151\n",
      "f1-score on set :  0.7397502207644757\n",
      "...Done.\n",
      "Accuracy on training set :  0.08239526517349426\n",
      "Accuracy on test set :  0.0875\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'constant', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7366185431779397\n",
      "f1-score on train set :  0.08239526517349426\n",
      "f1-score on test set :  0.0875\n",
      "f1-score on set :  0.7393437777120899\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630855840761344\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7632449131197718\n",
      "f1-score on train set :  0.7630855840761344\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7640462985290571\n",
      "...Done.\n",
      "Accuracy on training set :  0.6921215787086044\n",
      "Accuracy on test set :  0.7002688172043011\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 0.01}\n",
      "Best validation accuracy :  0.6911956502715186\n",
      "f1-score on train set :  0.6921215787086044\n",
      "f1-score on test set :  0.7002688172043011\n",
      "f1-score on set :  0.6921559289747601\n",
      "...Done.\n",
      "Accuracy on training set :  0.757265656979124\n",
      "Accuracy on test set :  0.7593386405388854\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'squared_hinge', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7570082710604633\n",
      "f1-score on train set :  0.757265656979124\n",
      "f1-score on test set :  0.7593386405388854\n",
      "f1-score on set :  0.7570024570024569\n",
      "...Done.\n",
      "Accuracy on training set :  0.7636605657237936\n",
      "Accuracy on test set :  0.7693229478729778\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'invscaling', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7569329233600127\n",
      "f1-score on train set :  0.7636605657237936\n",
      "f1-score on test set :  0.7693229478729778\n",
      "f1-score on set :  0.7621705058180955\n",
      "...Done.\n",
      "Accuracy on training set :  0.7571808692092515\n",
      "Accuracy on test set :  0.7593386405388854\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'squared_hinge', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7570189748660683\n",
      "f1-score on train set :  0.7571808692092515\n",
      "f1-score on test set :  0.7593386405388854\n",
      "f1-score on set :  0.7577281648675172\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630855840761344\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7628839124315847\n",
      "f1-score on train set :  0.7630855840761344\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7639934927999035\n",
      "...Done.\n",
      "Accuracy on training set :  0.7519902020820577\n",
      "Accuracy on test set :  0.7580743449116393\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'invscaling', 'eta0': 1, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7495309710795877\n",
      "f1-score on train set :  0.7519902020820577\n",
      "f1-score on test set :  0.7580743449116393\n",
      "f1-score on set :  0.7549805636540331\n",
      "...Done.\n",
      "Accuracy on training set :  0.758391085745346\n",
      "Accuracy on test set :  0.762660158633313\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7584582002921927\n",
      "f1-score on train set :  0.758391085745346\n",
      "f1-score on test set :  0.762660158633313\n",
      "f1-score on set :  0.7585911703558762\n",
      "...Done.\n",
      "Accuracy on training set :  0.758391085745346\n",
      "Accuracy on test set :  0.762660158633313\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7582393226531849\n",
      "f1-score on train set :  0.758391085745346\n",
      "f1-score on test set :  0.762660158633313\n",
      "f1-score on set :  0.758624908245657\n",
      "...Done.\n",
      "Accuracy on training set :  0.758405216328194\n",
      "Accuracy on test set :  0.762660158633313\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7586597098683165\n",
      "f1-score on train set :  0.758405216328194\n",
      "f1-score on test set :  0.762660158633313\n",
      "f1-score on set :  0.7586839530332682\n",
      "...Done.\n",
      "Accuracy on training set :  0.7420265311882586\n",
      "Accuracy on test set :  0.7460617517328293\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'squared_hinge', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7612233993651423\n",
      "f1-score on train set :  0.7420265311882586\n",
      "f1-score on test set :  0.7460617517328293\n",
      "f1-score on set :  0.7678920262830758\n",
      "...Done.\n",
      "Accuracy on training set :  0.7395333972515181\n",
      "Accuracy on test set :  0.7409948542024014\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'invscaling', 'eta0': 100, 'alpha': 0.01}\n",
      "Best validation accuracy :  0.7164335169900973\n",
      "f1-score on train set :  0.7395333972515181\n",
      "f1-score on test set :  0.7409948542024014\n",
      "f1-score on set :  0.7386828723800075\n",
      "...Done.\n",
      "Accuracy on training set :  0.7441282746160796\n",
      "Accuracy on test set :  0.7346938775510204\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'invscaling', 'eta0': 100, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.6433561070026032\n",
      "f1-score on train set :  0.7441282746160796\n",
      "f1-score on test set :  0.7346938775510204\n",
      "f1-score on set :  0.7593312139408925\n",
      "...Done.\n",
      "Accuracy on training set :  0.7627539049406717\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7629461760400972\n",
      "f1-score on train set :  0.7627539049406717\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7640395275970113\n",
      "...Done.\n",
      "Accuracy on training set :  0.7583067201195894\n",
      "Accuracy on test set :  0.7631257631257632\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7582898713579792\n",
      "f1-score on train set :  0.7583067201195894\n",
      "f1-score on test set :  0.7631257631257632\n",
      "f1-score on set :  0.758624908245657\n",
      "...Done.\n",
      "Accuracy on training set :  0.7545116407218625\n",
      "Accuracy on test set :  0.7586206896551724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'squared_hinge', 'learning_rate': 'invscaling', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7599803040661364\n",
      "f1-score on train set :  0.7545116407218625\n",
      "f1-score on test set :  0.7586206896551724\n",
      "f1-score on set :  0.7515891811043252\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630026809651475\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7628636572620298\n",
      "f1-score on train set :  0.7630026809651475\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7639365997709878\n",
      "...Done.\n",
      "Accuracy on training set :  0.7589104214663233\n",
      "Accuracy on test set :  0.7667071688942891\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7586468413697084\n",
      "f1-score on train set :  0.7589104214663233\n",
      "f1-score on test set :  0.7667071688942891\n",
      "f1-score on set :  0.7595878300103652\n",
      "...Done.\n",
      "Accuracy on training set :  0.758391085745346\n",
      "Accuracy on test set :  0.762660158633313\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7582705730978552\n",
      "f1-score on train set :  0.758391085745346\n",
      "f1-score on test set :  0.762660158633313\n",
      "f1-score on set :  0.7585911703558762\n",
      "...Done.\n",
      "Accuracy on training set :  0.7523561507936507\n",
      "Accuracy on test set :  0.7462850853054485\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'optimal', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.6891593518376655\n",
      "f1-score on train set :  0.7523561507936507\n",
      "f1-score on test set :  0.7462850853054485\n",
      "f1-score on set :  0.7478976976164241\n",
      "...Done.\n",
      "Accuracy on training set :  0.7588127419683488\n",
      "Accuracy on test set :  0.762660158633313\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7584074741425656\n",
      "f1-score on train set :  0.7588127419683488\n",
      "f1-score on test set :  0.762660158633313\n",
      "f1-score on set :  0.7586122498929205\n",
      "...Done.\n",
      "Accuracy on training set :  0.49633848657445084\n",
      "Accuracy on test set :  0.5104333868378812\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'constant', 'eta0': 1, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7341442894559973\n",
      "f1-score on train set :  0.49633848657445084\n",
      "f1-score on test set :  0.5104333868378812\n",
      "f1-score on set :  0.6861559139784946\n",
      "...Done.\n",
      "Accuracy on training set :  0.757265656979124\n",
      "Accuracy on test set :  0.7593386405388854\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'squared_hinge', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7574951108951306\n",
      "f1-score on train set :  0.757265656979124\n",
      "f1-score on test set :  0.7593386405388854\n",
      "f1-score on set :  0.7577281648675172\n",
      "...Done.\n",
      "Accuracy on training set :  0.7589104214663233\n",
      "Accuracy on test set :  0.7667071688942891\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.758544176097155\n",
      "f1-score on train set :  0.7589104214663233\n",
      "f1-score on test set :  0.7667071688942891\n",
      "f1-score on set :  0.7592897675269998\n",
      "...Done.\n",
      "Accuracy on training set :  0.7630026809651475\n",
      "Accuracy on test set :  0.7723723723723724\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7631626698932481\n",
      "f1-score on train set :  0.7630026809651475\n",
      "f1-score on test set :  0.7723723723723724\n",
      "f1-score on set :  0.7639759036144579\n",
      "...Done.\n",
      "Accuracy on training set :  0.7678571428571429\n",
      "Accuracy on test set :  0.7718475073313782\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'log_loss', 'learning_rate': 'invscaling', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7604389626742203\n",
      "f1-score on train set :  0.7678571428571429\n",
      "f1-score on test set :  0.7718475073313782\n",
      "f1-score on set :  0.7704218136278248\n",
      "...Done.\n",
      "Accuracy on training set :  0.7588127419683488\n",
      "Accuracy on test set :  0.762660158633313\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 1e-07}\n",
      "Best validation accuracy :  0.7584582002921927\n",
      "f1-score on train set :  0.7588127419683488\n",
      "f1-score on test set :  0.762660158633313\n",
      "f1-score on set :  0.7585911703558762\n",
      "...Done.\n",
      "Accuracy on training set :  0.758391085745346\n",
      "Accuracy on test set :  0.762660158633313\n",
      "Best hyperparameters :  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 100, 'alpha': 1e-08}\n",
      "Best validation accuracy :  0.7582206382121398\n",
      "f1-score on train set :  0.758391085745346\n",
      "f1-score on test set :  0.762660158633313\n",
      "f1-score on set :  0.758624908245657\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"Grid search...\")\n",
    "\n",
    "sgdc = SGDClassifier()\n",
    "\n",
    "loss = ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron']\n",
    "penalty = ['l2', 'elasticnet']\n",
    "alpha = [0.00000001, 0.0000001, 0.00000000001, 0.1]\n",
    "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "# class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
    "eta0 = [1, 10, 100]\n",
    "\n",
    "param_distributions = dict(loss=loss,\n",
    "                           penalty=penalty,\n",
    "                           alpha=alpha,\n",
    "                           learning_rate=learning_rate,\n",
    "                           # class_weight=class_weight,\n",
    "                           eta0=eta0)\n",
    "# >\n",
    "while True:\n",
    "    gridsearch_sgdc = RandomizedSearchCV(sgdc, param_distributions=param_distributions, cv=5, scoring=\"f1\", n_jobs=-1)\n",
    "                                      # verbose=True)  # cv : the number of folds to be used for CV\n",
    "    gridsearch_sgdc.fit(X_train, Y_train)\n",
    "    print(\"...Done.\")\n",
    "\n",
    "    print(\"Accuracy on training set : \", gridsearch_sgdc.score(X_train, Y_train))\n",
    "    print(\"Accuracy on test set : \", gridsearch_sgdc.score(X_test, Y_test))\n",
    "\n",
    "    # Predictions on training set\n",
    "    Y_train_pred = gridsearch_sgdc.predict(X_train)\n",
    "    Y_test_pred = gridsearch_sgdc.predict(X_test)\n",
    "\n",
    "\n",
    "    print(\"Best hyperparameters : \", gridsearch_sgdc.best_params_)\n",
    "    print(\"Best validation accuracy : \", gridsearch_sgdc.best_score_)\n",
    "\n",
    "    print(\"f1-score on train set : \", f1_score(Y_train, Y_train_pred))\n",
    "    print(\"f1-score on test set : \", f1_score(Y_test, Y_test_pred))\n",
    "    # f1-score on train set :  0.7678455758517944\n",
    "    # f1-score on test set :  0.769855072463768\n",
    "\n",
    "    # Concatenate our train and test set to train your best classifier on all data with labels\n",
    "\n",
    "    X = np.append(X_train, X_test, axis=0)\n",
    "    Y = np.append(Y_train, Y_test)\n",
    "\n",
    "    gridsearch_sgdc.best_estimator_.fit(X, Y)\n",
    "\n",
    "    pred = gridsearch_sgdc.best_estimator_.predict(X)\n",
    "    print(\"f1-score on set : \", f1_score(Y, pred))\n",
    "    a1 = f1_score(Y_train, Y_train_pred)\n",
    "    a2 = f1_score(Y_test, Y_test_pred)\n",
    "    a3 = f1_score(Y, pred)\n",
    "    # if a1, a2 and a3 are superior to 0.77\n",
    "\n",
    "    # create a file named randomsearch.txt and store the results\n",
    "    with open(\"randomsearch.txt\", \"a\") as f:\n",
    "        f.write(\"f1-score on train set : \" + str(a1) + \"\\n\")\n",
    "        f.write(\"f1-score on test set : \" + str(a2) + \"\\n\")\n",
    "        f.write(\"f1-score on set : \" + str(a3) + \"\\n\")\n",
    "        f.write(\"Best hyperparameters : \" + str(gridsearch_sgdc.best_params_) + \"\\n\")\n",
    "        f.write(\"Best validation accuracy : \" + str(gridsearch_sgdc.best_score_) + \"\\n\")\n",
    "        f.write(\"Accuracy on training set : \" + str(gridsearch_sgdc.score(X_train, Y_train)) + \"\\n\")\n",
    "        f.write(\"Accuracy on test set : \" + str(gridsearch_sgdc.score(X_test, Y_test)) + \"\\n\")\n",
    "        f.write(\"---------------------\")\n",
    "\n",
    "    if a1 > 0.77 and a2 > 0.77 and a3 > 0.77:\n",
    "        print(\"found\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train best classifier on all data and use it to make predictions on X_without_labels\n",
    "**Before making predictions on the file conversion_data_test.csv, let's train our model on ALL the data that was in conversion_data_train.csv. Sometimes, this allows to make tiny improvements in the score because we're using more examples to train the model.**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Concatenate our train and test set to train your best classifier on all data with labels\n",
    "X = np.append(X_train, X_test, axis=0)\n",
    "Y = np.append(Y_train, Y_test)\n",
    "\n",
    "gridsearch_sgdc.fit(X, Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = gridsearch_sgdc.predict(X)\n",
    "print(\"f1-score on set : \", f1_score(Y, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read data without labels\n",
    "data_without_labels = pd.read_csv('conversion_data_test.csv')\n",
    "print('Prediction set (without labels) :', data_without_labels.shape)\n",
    "\n",
    "# Warning : check consistency of features_list (must be the same than the features \n",
    "# used by your best classifier)\n",
    "features_list = ['country', 'age', 'new_user', 'source', 'total_pages_visited']\n",
    "X_without_labels = data_without_labels.loc[:, features_list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# WARNING : PUT HERE THE SAME PREPROCESSING AS FOR YOUR TEST SET\n",
    "# CHECK YOU ARE USING X_without_labels\n",
    "print(\"Encoding categorical features and standardizing numerical features...\")\n",
    "\n",
    "X_without_labels = preprocessor.transform(X_without_labels)\n",
    "print(\"...Done\")\n",
    "print(X_without_labels[0:5, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make predictions and dump to file\n",
    "# WARNING : MAKE SURE THE FILE IS A CSV WITH ONE COLUMN NAMED 'converted' AND NO INDEX !\n",
    "# WARNING : FILE NAME MUST HAVE FORMAT 'conversion_data_test_predictions_[name].csv'\n",
    "# where [name] is the name of your team/model separated by a '-'\n",
    "# For example : [name] = AURELIE-model1\n",
    "import datetime\n",
    "data = {\n",
    "    'converted': gridsearch_sgdc.predict(X_without_labels)\n",
    "}\n",
    "\n",
    "Y_predictions = pd.DataFrame(columns=['converted'], data=data)\n",
    "Y_predictions.to_csv(f'conversion_data_test_predictions_LV-{datetime.datetime.now().timestamp()}.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Projets_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0fdf8838102b5a6b2938995cfca801768491c1c6585082ea8435b0dad918ca1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}